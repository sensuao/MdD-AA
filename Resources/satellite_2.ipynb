{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF9uvbXNVrVY"
      },
      "source": [
        "## Import packages"
      ],
      "id": "zF9uvbXNVrVY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtPGh2MAVrVa"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "id": "rtPGh2MAVrVa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jlchl4x2VrVg"
      },
      "source": [
        "Import Tensorflow and the Keras classes needed to construct our model."
      ],
      "id": "Jlchl4x2VrVg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E82grprdYPI0"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf"
      ],
      "id": "E82grprdYPI0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1WtoaOHVrVh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import RMSprop, SDG\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "id": "L1WtoaOHVrVh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZZI6lNkVrVm"
      },
      "source": [
        "## Load data"
      ],
      "id": "UZZI6lNkVrVm"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCAjJLhr1fqC",
        "outputId": "833bf04b-98f1-42da-ca10-d7021352d5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "id": "HCAjJLhr1fqC"
    },
    {
      "cell_type": "code",
      "source": [
        "train_local_zip = '/content/drive/MyDrive/lulc-tra-val.zip'\n",
        "zip_ref = zipfile.ZipFile(train_local_zip, 'r')\n",
        "zip_ref.extractall('/content/sample_data/lulc-tra-val')\n",
        "\n",
        "test_local_zip = '/content/drive/MyDrive/test.zip'\n",
        "zip_ref = zipfile.ZipFile(test_local_zip, 'r')\n",
        "zip_ref.extractall('/content/sample_data/lulc-tra-val/lulc-classificatio')\n",
        "\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "DmYx9zPa25Mz"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DmYx9zPa25Mz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly_AzZbb0diz"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/sample_data/lulc-tra-val/lulc-classificatio'"
      ],
      "id": "ly_AzZbb0diz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpmywIlsVrVx"
      },
      "source": [
        "After extracting its contents, assign variables with the proper file path for the training and validation set."
      ],
      "id": "VpmywIlsVrVx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRucI3QqVrVy"
      },
      "outputs": [],
      "source": [
        "train_dir = os.path.join(PATH, 'train')\n",
        "validation_dir = os.path.join(PATH, 'validation')\n",
        "test_dir = os.path.join(PATH, 'test')"
      ],
      "id": "sRucI3QqVrVy"
    },
    {
      "cell_type": "code",
      "source": [
        "train_BarrenLands_dir = os.path.join(train_dir, '1_BarrenLands___jpeg')\n",
        "validation_BarrenLands_dir = os.path.join(validation_dir, '1_BarrenLands___jpeg')"
      ],
      "metadata": {
        "id": "u8swHNapWkBc"
      },
      "id": "u8swHNapWkBc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc4u8e9hVrV4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6145187-ae97-4e77-d170-9027dc79ba6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training barren images: 283\n",
            "total validation barren images: 71\n"
          ]
        }
      ],
      "source": [
        "num_barren_tr = len(os.listdir(train_BarrenLands_dir))\n",
        "num_barren_val = len(os.listdir(validation_BarrenLands_dir))\n",
        "\n",
        "print('total training barren images:', num_barren_tr)\n",
        "print('total validation barren images:', num_barren_val)"
      ],
      "id": "vc4u8e9hVrV4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NqNselLVrWA"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "epochs = 15\n",
        "img_height = 160\n",
        "img_width = 160"
      ],
      "id": "3NqNselLVrWA"
    },
    {
      "cell_type": "markdown",
      "id": "6HcE1TSqNRY2",
      "metadata": {
        "id": "6HcE1TSqNRY2"
      },
      "source": [
        "## Training and Validation Generators\n",
        "\n",
        "Now that you know the images you are dealing with, it is time for you to code the generators that will fed these images to your Network. For this, complete the `train_val_generators` function below:\n",
        "\n",
        "**Important Note:** The images have a resolution of 300x300 but the `flow_from_directory` method you will use allows you to set a target resolution. In this case, **set a `target_size` of (150, 150)**. This will heavily lower the number of trainable parameters in your final network, yielding much quicker training times without compromising the accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AX5Q3NL_FXMT",
      "metadata": {
        "cellView": "code",
        "id": "AX5Q3NL_FXMT"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: train_val_generators\n",
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "  ### START CODE HERE\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class \n",
        "  # Don't forget to normalize pixel values and set arguments to augment the images \n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
        "                                                      batch_size=batch_size, \n",
        "                                                      class_mode='categorical',\n",
        "                                                      target_size=(img_width, img_height))\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
        "  # Remember that validation data should not be augmented\n",
        "  validation_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      )\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
        "                                                                batch_size=batch_size, \n",
        "                                                                class_mode='categorical',\n",
        "                                                                target_size=(img_width, img_height))\n",
        "  ### END CODE HERE\n",
        "  return train_generator, validation_generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8FLUUqMKFwVR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FLUUqMKFwVR",
        "outputId": "16c58c80-52a5-451f-a640-e069580cdef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8207 images belonging to 29 classes.\n",
            "Found 2059 images belonging to 29 classes.\n"
          ]
        }
      ],
      "source": [
        "# Test your generators\n",
        "train_generator, validation_generator = train_val_generators(train_dir, validation_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TszKWhunQaj4",
      "metadata": {
        "id": "TszKWhunQaj4"
      },
      "source": [
        "**Expected Output:**\n",
        "```\n",
        "Found 1027 images belonging to 2 classes.\n",
        "Found 256 images belonging to 2 classes.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Izx51Ju1rXwd",
      "metadata": {
        "id": "Izx51Ju1rXwd"
      },
      "source": [
        "## Transfer learning - Create the pre-trained model\n",
        "\n",
        "Download the `inception V3` weights into the `/tmp/` directory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-lEzPAqxrPcU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lEzPAqxrPcU",
        "outputId": "5d8b05fe-b98d-40aa-e678-573413ad6eff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-03-27 10:23:27--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.133.128, 74.125.140.128, 108.177.15.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.133.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "/tmp/inception_v3_w 100%[===================>]  83.84M   164MB/s    in 0.5s    \n",
            "\n",
            "2022-03-27 10:23:28 (164 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the inception v3 weights\n",
        "#!wget --no-check-certificate \\\n",
        "#    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "#    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_zlXNulm9USZ",
      "metadata": {
        "id": "_zlXNulm9USZ"
      },
      "source": [
        "Now load the `InceptionV3` model and save the path to the weights you just downloaded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zfmRpsMf7E3-",
      "metadata": {
        "id": "zfmRpsMf7E3-"
      },
      "outputs": [],
      "source": [
        "# Import the inception model  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# Create an instance of the inception model from the local pre-trained weights\n",
        "#local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZPQb0PkT9_3w",
      "metadata": {
        "id": "ZPQb0PkT9_3w"
      },
      "source": [
        "Complete the `create_pre_trained_model` function below. You should specify the correct `input_shape` for the model (remember that you set a new resolution for the images instead of the native 300x300) and make all of the layers non-trainable:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "phE00SCr-RCT",
      "metadata": {
        "id": "phE00SCr-RCT"
      },
      "source": [
        "Check that everything went well by comparing the last few rows of the model summary to the expected output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rs0pvgMXpG_",
        "outputId": "d977797f-0505-44cc-98b5-d694f8f4b145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 4s 0us/step\n",
            "87924736/87910968 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# GRADED FUNCTION: create_pre_trained_model\n",
        "#def create_pre_trained_model(local_weights_file):\n",
        "  ### START CODE HERE\n",
        "pre_trained_model = InceptionV3(input_shape = (img_width, img_height, 3),\n",
        "                                  include_top = False, \n",
        "                                  weights = 'imagenet') \n",
        "\n",
        "  #pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  # Make all the layers in the pre-trained model non-trainable\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "  ### END CODE HERE\n",
        "\n",
        "  #return pre_trained_model\n",
        "  "
      ],
      "id": "6Rs0pvgMXpG_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndfH0H1PXpHF"
      },
      "outputs": [],
      "source": [
        "#pre_trained_model.trainable = False\n",
        "#pre_trained_model = create_pre_trained_model(local_weights_file)\n",
        "\n",
        "# Print the model summary\n",
        "pre_trained_model.summary()"
      ],
      "id": "ndfH0H1PXpHF"
    },
    {
      "cell_type": "markdown",
      "id": "4cAY2gQytr0-",
      "metadata": {
        "id": "4cAY2gQytr0-"
      },
      "source": [
        "**Expected Output:**\n",
        "```\n",
        "batch_normalization_v1_281 (Bat (None, 3, 3, 192)    576         conv2d_281[0][0]                 \n",
        "__________________________________________________________________________________________________\n",
        "activation_273 (Activation)     (None, 3, 3, 320)    0           batch_normalization_v1_273[0][0] \n",
        "__________________________________________________________________________________________________\n",
        "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_275[0][0]             \n",
        "                                                                activation_276[0][0]             \n",
        "__________________________________________________________________________________________________\n",
        "concatenate_5 (Concatenate)     (None, 3, 3, 768)    0           activation_279[0][0]             \n",
        "                                                                activation_280[0][0]             \n",
        "__________________________________________________________________________________________________\n",
        "activation_281 (Activation)     (None, 3, 3, 192)    0           batch_normalization_v1_281[0][0] \n",
        "__________________________________________________________________________________________________\n",
        "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_273[0][0]             \n",
        "                                                                mixed9_1[0][0]                   \n",
        "                                                                concatenate_5[0][0]              \n",
        "                                                                activation_281[0][0]             \n",
        "==================================================================================================\n",
        "Total params: 21,802,784\n",
        "Trainable params: 0\n",
        "Non-trainable params: 21,802,784\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MRHkV9jo-hkh",
      "metadata": {
        "id": "MRHkV9jo-hkh"
      },
      "source": [
        "To check that all the layers in the model were set to be non-trainable, you can also run the cell below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VASOaB8xDbhU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VASOaB8xDbhU",
        "outputId": "251742f4-64a8-453b-9974-fca274d230e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 21,802,784 total parameters in this model.\n",
            "There are 0 trainable parameters in this model.\n"
          ]
        }
      ],
      "source": [
        "total_params = pre_trained_model.count_params()\n",
        "num_trainable_params = sum([w.shape.num_elements() for w in pre_trained_model.trainable_weights])\n",
        "\n",
        "print(f\"There are {total_params:,} total parameters in this model.\")\n",
        "print(f\"There are {num_trainable_params:,} trainable parameters in this model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mRioO7FH5a8I",
      "metadata": {
        "id": "mRioO7FH5a8I"
      },
      "source": [
        "**Expected Output:**\n",
        "```\n",
        "There are 21,802,784 total parameters in this model.\n",
        "There are 0 trainable parameters in this model.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dFtwDyKj-4GR",
      "metadata": {
        "id": "dFtwDyKj-4GR"
      },
      "source": [
        "## Creating callbacks for later\n",
        "\n",
        "You have already worked with callbacks in the first course of this specialization so the callback to stop training once an accuracy of 99.9% is reached, is provided for you:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lHZnFl-5_p3a",
      "metadata": {
        "id": "lHZnFl-5_p3a"
      },
      "source": [
        "## Pipelining the pre-trained model with your own\n",
        "\n",
        "Now that the pre-trained model is ready, you need to \"glue\" it to your own model to solve the task at hand.\n",
        "\n",
        "For this you will need the last output of the pre-trained model, since this will be the input for your own. Complete the `output_of_last_layer` function below.\n",
        "\n",
        "**Note:** For grading purposes use the `mixed7` layer as the last layer of the pre-trained model. However, after submitting feel free to come back here and play around with this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CFsUlwdfs_wg",
      "metadata": {
        "id": "CFsUlwdfs_wg"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: output_of_last_layer\n",
        "def output_of_last_layer(pre_trained_model):\n",
        "  ### START CODE HERE\n",
        "  last_desired_layer = pre_trained_model.get_layer('mixed9')\n",
        "  print('last layer output shape: ', last_desired_layer.output_shape)\n",
        "  last_output = last_desired_layer.output\n",
        "  print('last layer output: ', last_output)\n",
        "  ### END CODE HERE\n",
        "\n",
        "  return last_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13AEzKG2A6_J",
      "metadata": {
        "id": "13AEzKG2A6_J"
      },
      "source": [
        "Check that everything works as expected:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zOJPUtMN6PHo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOJPUtMN6PHo",
        "outputId": "b76b97cd-91b1-4298-fe41-9831bdc0a5d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last layer output shape:  (None, 3, 3, 2048)\n",
            "last layer output:  KerasTensor(type_spec=TensorSpec(shape=(None, 3, 3, 2048), dtype=tf.float32, name=None), name='mixed9/concat:0', description=\"created by layer 'mixed9'\")\n"
          ]
        }
      ],
      "source": [
        "last_output = output_of_last_layer(pre_trained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XqIWKZ_h7CuY",
      "metadata": {
        "id": "XqIWKZ_h7CuY"
      },
      "source": [
        "**Expected Output (if `mixed7` layer was used):**\n",
        "```\n",
        "last layer output shape:  (None, 7, 7, 768)\n",
        "last layer output:  KerasTensor(type_spec=TensorSpec(shape=(None, 7, 7, 768), dtype=tf.float32, name=None), name='mixed7/concat:0', description=\"created by layer 'mixed7'\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Rp-J6JuwJTq",
      "metadata": {
        "id": "0Rp-J6JuwJTq"
      },
      "source": [
        "Now you will create the final model by adding some additional layers on top of the pre-trained model.\n",
        "\n",
        "Complete the `create_final_model` function below. You will need to use Tensorflow's [Functional API](https://www.tensorflow.org/guide/keras/functional) for this since the pretrained model has been created using it. \n",
        "\n",
        "Let's double check this first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cKQknB4j7K9y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKQknB4j7K9y",
        "outputId": "68cdd07b-6b50-44f8-bc9d-7317aee01110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The pretrained model has type: <class 'keras.engine.functional.Functional'>\n"
          ]
        }
      ],
      "source": [
        "# Print the type of the pre-trained model\n",
        "print(f\"The pretrained model has type: {type(pre_trained_model)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kt7AU7jP7LW9",
      "metadata": {
        "id": "Kt7AU7jP7LW9"
      },
      "source": [
        "To create the final model, you will use Keras' Model class by defining the appropriate inputs and outputs as described in the first way to instantiate a Model in the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n",
        "\n",
        "Note that you can get the input from any existing model by using its `input` attribute and by using the Funcional API you can use the last layer directly as output when creating the final model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BMXb913pbvFg",
      "metadata": {
        "cellView": "code",
        "id": "BMXb913pbvFg"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: create_final_model\n",
        "def create_final_model(pre_trained_model, last_output):\n",
        "  # Flatten the output layer to 1 dimension\n",
        "  x = layers.Flatten()(last_output)\n",
        "\n",
        "  ### START CODE HERE\n",
        "\n",
        "  # Add a fully connected layer with 1024 hidden units and ReLU activation\n",
        "  x = layers.Dense(64, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.4)(x)   \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense(29, activation='softmax')(x)\n",
        "\n",
        "\n",
        "  # Create the complete model by using the Model class\n",
        "  model = Model(inputs=pre_trained_model.input, outputs=x)\n",
        "\n",
        "  # Compile the model\n",
        "  base_learning_rate = 0.001\n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cL6ga5Z1783H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cL6ga5Z1783H",
        "outputId": "10bf5d21-c704-4d52-844e-56d9d2c04395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 16,904,317 total parameters in this model.\n",
            "There are 1,181,597 trainable parameters in this model.\n"
          ]
        }
      ],
      "source": [
        "# Save your model in a variable\n",
        "model = create_final_model(pre_trained_model, last_output)\n",
        "\n",
        "# Inspect parameters\n",
        "total_params = model.count_params()\n",
        "num_trainable_params = sum([w.shape.num_elements() for w in model.trainable_weights])\n",
        "\n",
        "print(f\"There are {total_params:,} total parameters in this model.\")\n",
        "print(f\"There are {num_trainable_params:,} trainable parameters in this model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "J4d3zlcQDrvm",
      "metadata": {
        "id": "J4d3zlcQDrvm"
      },
      "source": [
        "**Expected Output:**\n",
        "```\n",
        "There are 47,512,481 total parameters in this model.\n",
        "There are 38,537,217 trainable parameters in this model.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_eqwHj5xEBZ7",
      "metadata": {
        "id": "_eqwHj5xEBZ7"
      },
      "source": [
        "Wow, that is a lot of parameters!\n",
        "\n",
        "After submitting your assignment later, try re-running this notebook but use the original resolution of 300x300, you will be surprised to see how many more parameters are for that case.\n",
        "\n",
        "Now train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SeVjZD2o7gWS"
      },
      "outputs": [],
      "source": [
        "#Checkpoint In the /output folder\n",
        "filepath = '/content/sample_data/lulc-tra-val/best_model_lulc.hdf5'\n",
        "# Keep only a single checkpoint, the best over test accuracy.\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
      ],
      "id": "SeVjZD2o7gWS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Blhq2MAUeyGA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blhq2MAUeyGA",
        "outputId": "06b0ab82-136b-40e5-d0df-9dc95ed2b76b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "129/129 [==============================] - ETA: 0s - loss: 2.4429 - accuracy: 0.3041\n",
            "Epoch 1: val_accuracy did not improve from 0.52161\n",
            "129/129 [==============================] - 75s 542ms/step - loss: 2.4429 - accuracy: 0.3041 - val_loss: 1.8763 - val_accuracy: 0.4109\n",
            "Epoch 2/10\n",
            "129/129 [==============================] - ETA: 0s - loss: 1.9099 - accuracy: 0.4182\n",
            "Epoch 2: val_accuracy did not improve from 0.52161\n",
            "129/129 [==============================] - 70s 543ms/step - loss: 1.9099 - accuracy: 0.4182 - val_loss: 1.5887 - val_accuracy: 0.4896\n",
            "Epoch 3/10\n",
            "129/129 [==============================] - ETA: 0s - loss: 1.7388 - accuracy: 0.4584\n",
            "Epoch 3: val_accuracy did not improve from 0.52161\n",
            "129/129 [==============================] - 70s 543ms/step - loss: 1.7388 - accuracy: 0.4584 - val_loss: 1.5088 - val_accuracy: 0.5158\n",
            "Epoch 4/10\n",
            "129/129 [==============================] - ETA: 0s - loss: 1.6348 - accuracy: 0.4921\n",
            "Epoch 4: val_accuracy did not improve from 0.52161\n",
            "129/129 [==============================] - 70s 539ms/step - loss: 1.6348 - accuracy: 0.4921 - val_loss: 1.5338 - val_accuracy: 0.4871\n",
            "Epoch 5/10\n",
            "129/129 [==============================] - ETA: 0s - loss: 1.5765 - accuracy: 0.4964\n",
            "Epoch 5: val_accuracy improved from 0.52161 to 0.52793, saving model to /content/sample_data/lulc-tra-val/best_model_lulc.hdf5\n",
            "129/129 [==============================] - 69s 534ms/step - loss: 1.5765 - accuracy: 0.4964 - val_loss: 1.4611 - val_accuracy: 0.5279\n",
            "Epoch 6/10\n",
            "129/129 [==============================] - ETA: 0s - loss: 1.5113 - accuracy: 0.5241\n",
            "Epoch 6: val_accuracy improved from 0.52793 to 0.53424, saving model to /content/sample_data/lulc-tra-val/best_model_lulc.hdf5\n",
            "129/129 [==============================] - 69s 537ms/step - loss: 1.5113 - accuracy: 0.5241 - val_loss: 1.4233 - val_accuracy: 0.5342\n",
            "Epoch 7/10\n",
            "129/129 [==============================] - ETA: 0s - loss: 1.4899 - accuracy: 0.5260\n",
            "Epoch 7: val_accuracy improved from 0.53424 to 0.54687, saving model to /content/sample_data/lulc-tra-val/best_model_lulc.hdf5\n",
            "129/129 [==============================] - 70s 539ms/step - loss: 1.4899 - accuracy: 0.5260 - val_loss: 1.4337 - val_accuracy: 0.5469\n",
            "Epoch 8/10\n",
            "129/129 [==============================] - ETA: 0s - loss: 1.4467 - accuracy: 0.5380\n",
            "Epoch 8: val_accuracy improved from 0.54687 to 0.55707, saving model to /content/sample_data/lulc-tra-val/best_model_lulc.hdf5\n",
            "129/129 [==============================] - 73s 564ms/step - loss: 1.4467 - accuracy: 0.5380 - val_loss: 1.3941 - val_accuracy: 0.5571\n",
            "Epoch 9/10\n",
            "129/129 [==============================] - ETA: 0s - loss: 1.4248 - accuracy: 0.5381\n",
            "Epoch 9: val_accuracy improved from 0.55707 to 0.56387, saving model to /content/sample_data/lulc-tra-val/best_model_lulc.hdf5\n",
            "129/129 [==============================] - 71s 546ms/step - loss: 1.4248 - accuracy: 0.5381 - val_loss: 1.3744 - val_accuracy: 0.5639\n",
            "Epoch 10/10\n",
            "129/129 [==============================] - ETA: 0s - loss: 1.4019 - accuracy: 0.5503\n",
            "Epoch 10: val_accuracy did not improve from 0.56387\n",
            "129/129 [==============================] - 69s 533ms/step - loss: 1.4019 - accuracy: 0.5503 - val_loss: 1.4073 - val_accuracy: 0.5517\n"
          ]
        }
      ],
      "source": [
        "# Run this and see how many epochs it should take before the callback\n",
        "# fires, and stops training at 99.9% accuracy\n",
        "# (It should take a few epochs)\n",
        "#callbacks = myCallback()\n",
        "history = model.fit(train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    #steps_per_epoch = num_barren_tr // batch_size,\n",
        "                    epochs = 10,\n",
        "                    #validation_steps = num_barren_val // batch_size,\n",
        "                    verbose = 1,\n",
        "                    callbacks=checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y94djl4t0sK5",
      "metadata": {
        "id": "Y94djl4t0sK5"
      },
      "source": [
        "The training should have stopped after less than 10 epochs and it should have reached an accuracy over 99,9% (firing the callback). This happened so quickly because of the pre-trained model you used, which already contained information to classify humans from horses. Really cool!\n",
        "\n",
        "Now take a quick look at the training and validation accuracies for each epoch of training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C2Fp6Se9rKuL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "C2Fp6Se9rKuL",
        "outputId": "5114da2c-03f3-405f-de4e-c6c6bb1331d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot the training and validation accuracies for each epoch\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g-4-4i9U1a0s",
      "metadata": {
        "id": "g-4-4i9U1a0s"
      },
      "source": [
        "You will need to submit this notebook for grading. To download it, click on the `File` tab in the upper left corner of the screen then click on `Download` -> `Download .ipynb`. You can name it anything you want as long as it is a valid `.ipynb` (jupyter notebook) file."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7w54-pbB1W9r",
      "metadata": {
        "id": "7w54-pbB1W9r"
      },
      "source": [
        "**Congratulations on finishing this week's assignment!**\n",
        "\n",
        "You have successfully implemented a convolutional neural network that leverages a pre-trained network to help you solve the problem of classifying humans from horses.\n",
        "\n",
        "**Keep it up!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "satellite_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}